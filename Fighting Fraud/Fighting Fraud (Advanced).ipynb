{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Case Study: Fighting Fraud}}$</h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/banner2.png\" alt=\"Tec banner\" style=\"display: auto;margin-left:auto;margin-right: auto;width:120%;\"/>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Advanced Version}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">This is an advanced version. It includes no code whatsoever; however, it includes a lot of comments to help you along the study case. Follow the links to functions, modules and libraries documentations. If you get stuck on a particular part, feel free to also use the beginner version in $\\color{blue}{\\textbf{Fighting Fraud (Beginners).ipynb}}$ to help you out.<br><br>$\\color{red}{\\textbf{Attention!!!}}$<br>Whenever you see this image <img src=\"Images/broke.jpg\" width=\"300\" align=\"Center\"/><br> it means that $\\color{red}{\\textbf{code is missing or an answer is required}}$ and you must fill these gaps.<br><br>$\\color{red}{\\textbf{Note: If you close the notebook, you will have to run all cells again upon-reopening it}}$.<br>   \n",
    "\n",
    "<img src=\"Images/luck.jpg\" width=\"100\" align=\"Left\"/><h2><br>$\\,\\,\\,\\color{green}{\\textbf{Good luck solving this case!!! Have fun!!!}}$</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{blue}{\\textbf{Identification}}$\n",
    "<table style=\"width:100%\" align=\"right\">\n",
    "  <tr>\n",
    "    <th><h2>$\\color{black}{\\textbf{Name}}$</h2></th>\n",
    "    <th><h2>$\\color{black}{\\textbf{Student ID}}$</h2></th>\n",
    "  </tr>\n",
    "    \n",
    "  <tr>\n",
    "    <td><h3>$\\color{black}{\\text{Name}}$</h3></td>\n",
    "    <td><h3>$\\color{black}{\\text{ID}}$</h3></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><h3>$\\color{black}{\\text{Name}}$</h3></td>\n",
    "    <td><h3>$\\color{black}{\\text{ID}}$</h3></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td><h3>$\\color{black}{\\text{Name}}$</h3></td>\n",
    "    <td><h3>$\\color{black}{\\text{ID}}$</h3></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td><h3>$\\color{black}{\\text{Name}}$</h3></td>\n",
    "    <td><h3>$\\color{black}{\\text{ID}}$</h3></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Introduction}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The total losses on UK cards (debit and credit) increased to ¬£185 million in the first half of 2012, a 9% rise compared with the last six months of 2011 (source: UK Cards Association).<br><br>Although this is below the peak of ¬£304 million in losses in the first half of 2008, criminals are increasingly resorting to traditional ways of stealing people's details to combat anti-fraud advances such as chip and PIN.</p>\n",
    "\n",
    "\n",
    "<center style=\"font-weight: bold; font-size: 150%;\">!! And the situation has only worsened in recent years !!</center>\n",
    "\n",
    "<img src=\"Images/creditfraud.jpg\" alt=\"Fraud\" style=\"display: auto;margin-left:auto;margin-right: auto;width:40%;\"/>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Unauthorised financial fraud losses across payment cards, remote banking and cheques totalled ¬£844.8 million in 2018, an increase of 16% compared to 2017. In addition, in 2018 finance industry members reported 84,624 incidents of authorised push payment scams with gross losses of ¬£354.3 million.<br><br>In this project we will load and explore a dataset that contains transactions made by credit cards in September 2013 by European cardholders. This dataset present transactions that occurred over the course of two days, where (according to [Universit√© Libre de Bruxelles](https://mlg.ulb.ac.be/wordpress/)) 492 frauds were detected out of the 284,807 transactions.<br><br>Part of the challenge will be dealing with such a large dataset (one which, for example, usually causes Microsoft Excel to crash). Ultimately our aim is to compare and contrast various machine learning algorithms - in order that we may find those 492 fraudulent transactions ourselves.<br><br>In particular, the project will allow us to investigate methods of dealing with a highly unbalanced dataset, where the positive class (fraudulent transactions) account for just 0.172% of all transactions - thereby (possibly!) rendering many common and well known machine learning algorithms ineffective.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Python Imports and Setup}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Run the following cell to import all the libraries you need to complete the case study into the Jupyter Notebook.<br><br>The first command simply ensures that any plot we produce in this notebook is immediately displayed on screen, directly below the code that created it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    import scipy.stats as stats\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "except ModuleNotFoundError as err:\n",
    "    print(\"Python says:\", err, \"... possibly because it isn't installed on your system?\")\n",
    "except:\n",
    "    print(\"Some other error occurred when importing.\")\n",
    "else:\n",
    "    print(\"Imports successful!\")\n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{blue}{\\textbf{The Dataset}}$\n",
    "\n",
    "</br></br>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The dataset we will be using for this project is a CSV file named $\\color{blue}{\\texttt{creditcard.csv}}$, which you can find on our [GitHub repository](https://github.com/semana-i-2019) in the $\\color{blue}{\\texttt{Fighting Fraud}}$ folder.\n",
    "\n",
    "### $\\color{red}{\\textbf{Note: Since the file is 66 MB, do not try and print its contents in a Notebook cell:}}$\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">This will either crash the Notebook, or take too long to process, or take up a rididculous amount of space in your Notebook (which you do not want to have to take forever to scroll through). Besides, getting a grasp of what your data looks like is part of the challenge of data exploration!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Original-DataFrame'></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Read the data}}$</h1>\n",
    "\n",
    "### <br>[Read the file](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) $\\color{blue}{\\texttt{creditcard.csv}}$ and name it ${\\color{blue}{\\texttt{df}\\_{\\text{orig}}}}$ as in \"DataFrame Original\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/exploratory.png\" alt=\"Tec banner\" style=\"display: auto;margin-left:auto;margin-right: auto;width:150%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p align=\"justify\" style=\"line-height:25px\"> We are lucky that the dataset has already been cleaned and properly organised to allow it to be read straight into a pandas DataFrame data structure. Thus, it is already structured into rows and columns $\\dots$ much like a Microsoft Excel spreadsheet $\\dots$ and those columns are appropriately named. However, it is still invaluable to thoroughly explore the dataset to be totally satisfied. For example, how can we be completely confident there are no $\\color{blue}{\\texttt{n/a}}$ or $\\color{blue}{\\texttt{nan}}$ to deal with?</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{ùêêùêÆùê¢ùêúùê§ ùêÉùêöùê≠ùêöùêÖùê´ùêöùê¶ùêû ùê¨ùêÆùê¶ùê¶ùêöùê´ùê≤}}$</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{How many transactions and variables are there?}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Use the [pandas shape](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html>) attribute to return the dimensions of the DataFrame.<br><br>This command returns a [tuple](<https://docs.python.org/3.3/tutorial/datastructures.html#tuples-and-sequences>) that represents the dimensionality of the DataFrame - that is, the number of rows, and number of columns.<br><br>Note how we can access the rows and then the columns separately by using the index operator (the square brackets) with $0$ for rows, and $1$ for columns. Again, as noted above, try to think of the DataFrame as if it were a Microsoft Excel spreadsheet.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='info'></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{How can we get a summary of the DataFrame?}}$</h1>\n",
    "\n",
    "### <br> <p align=\"justify\" style=\"line-height:25px\">Particularly, let us get [information about our DataFrame's](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html>) column names, column data types, and whether there are any null ($\\color{blue}{\\texttt{n/a}}$ or $\\color{blue}{\\texttt{nan}}$) values to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/>\n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{A random sample of n transactions}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Since the dataset is so big, printing it out here (or indeed trying to open it in Microsoft Excel) is either impractical or just not possible. In which case, we can ask pandas to show us a random sample of n rows (all columns) using the [sample method](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html>).<br><br> The $\\color{blue}{\\texttt{axis}}$ you choose defaults to $\\color{blue}{\\texttt{None}}$, which fallsback to the data type of the object ([${\\color{blue}{\\texttt{df}\\_{\\text{orig}}}}$](#Original-DataFrame)) that you're querying ( $0$ for Series and DataFrames, $1$ for Panels). Thus, to be specific, we explicitly choose axis $0$, so that we get back a random sampling of rows from our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/an1.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Anonymised Data}}$</h1>\n",
    "    \n",
    "### <p style=\"line-height:25px\">  We're dealing with a dataset of credit card transactions, but for data protection purposes, the data has been anonymised, which is why we see column labels such as V1, V2, etc. And it is also why, unfortunately, we probably wont be able to say much more about these columns nor draw many more conclusions from them.<br><br>After a transaction is authorized, it becomes a vector with few variables such as $\\color{blue}{\\texttt{Card}\\_{\\texttt{Id}}}$, $\\color{blue}{\\texttt{Amount}}$, $\\color{blue}{\\texttt{Date}}$, $\\color{blue}{\\texttt{Merchant}}$, $\\color{blue}{\\texttt{Product}}$ and so on. New features are aggregated, features such as number of daily transactions, weekly average expenditure, etc., and then merged with the original features to form a new vector which is analyzed in typically less than six seconds.<br><br>Due to confidentiality issues, banks cannot provide the original features and more background information about the data. Features V1, V2,$\\dots$, V28 are the principal components obtained with a [Principal Component Analysis (PCA)](https://www.sciencedirect.com/topics/engineering/principal-component-analysis) transformation. The only features which have not been transformed with PCA are $\\color{blue}{\\texttt{Time}}$, $\\color{blue}{\\texttt{Amount}}$ and $\\color{blue}{\\texttt{Class}}$.\n",
    "    \n",
    "<img src=\"Images/pca.jpeg\" alt=\"PCA\" style=\"display: auto;margin-left:auto;margin-right: auto;width:35%;\"/>\n",
    "    \n",
    "### <p style=\"line-height:25px\">Principal component analysis (PCA) is a multivariate analysis technique, which transforms original data into new uncorrelated variables called principal components (PCs). The original data are treated as independent variables and each PC is a linear combination of these original variables. The PCs form the basis of a vector space and they are arranged in order of decreasing variance. Thus the first PC carries the most information regarding the original data, and so on.<br><br>Apart from producing derived variables for use in supervised learning problems, PCA produces a low-dimensional representation of a data set; it also serves as a tool for data visualization.\n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{DataFrame Descriptive Statistics}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Generate descriptive statistics that summarize the DataFrame values.<br><br>The feature $\\color{blue}{\\texttt{Class}}$ is the response variable and it takes a value $1$ in case of fraud and $0$ otherwise; therefore, exclude the column $\\color{blue}{\\texttt{Class}}$ from the report, since reporting descriptive statistics does not make any sense for this column.<br> $\\color{red}{\\textbf{Hint}}$: You want to exclude an [int64-type object](#info).<br><br>The pandas method [describe](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html>) analyzes both numeric and object data types, as well as DataFrame column sets of mixed data types.<br><br>From the profile report above we know that the data types are all numeric - so we should expect $\\color{blue}{\\texttt{describe}}$ to return, for each column, a standard five-number summary (minimum, lower quartile, median, upper quartile, maximum) as well as a report on the dataset's dispersion, central tendency, and a count of how many rows there are.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Transaction-Amount-and-Time-Descriptive-Statistics'></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Focus on Time and Amount}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"><br>Feature $\\color{blue}{\\texttt{Time}}$ is the number of seconds elapsed between a particular transaction and the first transaction in the dataset. The feature $\\color{blue}{\\texttt{Amount}}$ is just the transaction amount.<br><br>Columns $\\color{blue}{\\texttt{Amount}}$ and $\\color{blue}{\\texttt{Time}}$ may well provide us with useful information about the spending habits of the \"average\" credit card holder. In turn, this might prove useful for spotting any (fraudulent?) outliers, such as:<br><br>$\\bullet$ Particularly high expenditures - perhaps suggesting a one off money grab by a fraudster?<br><br>$\\bullet$ Obscure spending patterns such as multiple amounts of the same value - perhaps suggesting an automated bot-transaction has been set in action to skim funds from the account holder, without drawing attention to itself?<br><br> Generate the descriptive statistics of these two columns using the [pandas function loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html): Focus on all rows (the colon) for column labels $\\color{blue}{\\texttt{Time}}$ and $\\color{blue}{\\texttt{Amount}}$ only (i.e., non-anonymized columns). Then run $\\color{blue}{\\texttt{describe()}}$ on these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Fraudulent and non-fraudulent transactions}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The column $\\color{blue}{\\texttt{Class}}$ (case-sensitive) contains either a $0$, meaning a regular, non-fraudulent transaction, or a $1$, meaning a fraudulent transaction.<br><br>1. Extract the column class from the DataFrame. Simply typing $\\color{blue}{\\texttt{df}\\_\\texttt{orig.Class}}$ into a Notebook cell will return a [pandas Series object](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html>), which you can think of as a column of data, where the rows have been labelled (with row numbers). This will contain only the column labelled $\\color{blue}{\\texttt{Class}}$.<br><br>2. Next, rather than saving the above Series object to a variable, \"chain\" a second command onto the end of the $\\color{blue}{\\texttt{df}\\_\\texttt{orig.Class}}$ command above. This second command is the [counts method](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html>), which returns another pandas Series object, only this time containing a count of the occurence of unique values from the original $\\color{blue}{\\texttt{Class}}$ column.<br><br><a id='Fraudulent-non-fraudulent-transactions'></a> 3. Save this chain to a variable called $\\color{blue}{\\texttt{counts}}$. Thus, each unique value from the original $\\color{blue}{\\texttt{Class}}$ column will be given it's own row  with a value that equals the number of times it appears in the original.<br><br> 4. There will be as many rows in this new Series object as there are unique values in the original $\\color{blue}{\\texttt{Class}}$ column. Thus, again, you should expect there to be only two types of elements in the variable $\\color{blue}{\\texttt{counts}}$, one for fraudulent transactions (of value $1$) and one for non-fraudulent transactions (of value $0$). Double check this by using $\\color{blue}{\\texttt{len(counts)}}$, which should equal $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Fraudulent vs non-fraudulent transactions}}$</h1>\n",
    "\n",
    "###  <p align=\"justify\" style=\"line-height:25px\">1. Use the variable [$\\color{blue}{\\texttt{counts}}$](#Fraudulent-non-fraudulent-transactions) to count the number of fraudulent and non-fraudulent transactions, and save these counts on variables called ${\\color{blue}{\\texttt{fraud}\\_{\\texttt{count}}}}$ and ${\\color{blue}{\\texttt{non}\\_{\\texttt{fraud}\\_{\\texttt{count}}}}}$, respectively. Use square brackets to select items from [$\\color{blue}{\\texttt{counts}}$](#Fraudulent-non-fraudulent-transactions) that equal whatever is in the bracket; remember, $0$ stands for non-fraudulent and $1$ for fraudulent.<br><br>2. Check that the total number of transactions equals the sum of the number of fraudulent and non-fraudulent transactions.<br><br>3. Print the number of fraudulent and non-fraudulent transactions together with their percentages.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\color{blue}{\\textbf{Visual Data Exploration}}$\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Numerical data exploration is not the only way we can explore our dataset. Indeed, in many cases, if you only relied on numerical exploration you might miss important details in the data that could mislead you into drawing entirely the wrong conclusions about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Transaction Time data distribution}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Let us deal with just the Transaction Time column of data here - hence, formally, this can be described as a univariate distribution of observations. Thus, head to the [seaborn API](<http://seaborn.pydata.org/api.html#distribution-plots>) and have a look at the range of distribution plots available to us.<br><br>1. Produce either a histogram or a kernel density plot (better both!) of the distribution of Transaction Time data<br><br>2. Describe the distribution you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Transaction Time distribution description}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Note that the distribution plot shows a $\\color{red}{\\underline{\\text{answer}}}$ distribution. Thus from the point of view of a general statistical description it turns out that although several summary statistics have been suggested to describe such distributions, there is presently no agreed set of statistics that can properly quantify the parameters of [such a general distribution](https://www.thoughtco.com/definition-of-bimodal-in-statistics-3126325) $\\dots\\dots\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{So what conclusion(s) can we draw ourselves?}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">1. What is the maximum time value? What does it represent?<br><br>$\\color{red}{\\textbf{Answer}}$<br><br>2. Reconcile your answer in part 1 with the distribution you obtained in the previous cell; precisely, what does the transaction time distribution represent?<br><br>$\\color{red}{\\textbf{Answer}}$<br><br>3. How could you explain the continued credit card activity through the night?<br><br>$\\color{red}{\\textbf{Answer}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Transaction Amount data distribution}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Now let us deal with the Transaction Amount column of data here.<br><br>1. Produce either a histogram or kernel density plot (better both!) of the distribution of Transaction Amount data<br><br>2. Describe the distribution you obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Transaction Amount distribution description}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">As noted above when first describing [transaction amount and time](#Transaction-Amount-and-Time-Descriptive-Statistics) with descriptive statistics - out of all credit card transactions:<br><br>* The mean was shown to be $\\color{red}{\\underline{\\text{answer}}}$ USD - set against a median of $\\color{red}{\\underline{\\text{answer}}}$ USD.<br><br>* The standard deviation was shown to be $\\color{red}{\\underline{\\text{answer}}}$ USD.<br><br>* The maximum transaction was shown to be $\\color{red}{\\underline{\\text{answer}}}$ USD.<br><br>Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. As a rule of thumb we would normally expect the mean to be to the right of the median when the distribution is right skewed, and to the left of the median when the distribution has left skew. This, alongside a visual inspection of the above figure, clearly shows the distribution of transaction amount has a $\\color{red}{\\underline{\\text{answer}}}$ skew, i.e. it is $\\color{red}{\\underline{\\text{answer}}}-$ skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Skewness}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">As an aside, compare the skewness of Transaction Time and Amount to their respective distribution plots:<br><br>The distribution plot of transaction amount showed a highly $\\color{red}{\\underline{\\text{answer}}}$ skew.<br><br>The distribution plot of transaction time showed a more complex $\\color{red}{\\underline{\\text{answer}}}$ distribution and thus (as we've mentioned already) although several summary statistics have been suggested to describe such distributions, there is no presently generally agreed set of statistics that can properly quantify the parameters of a general bimodal distribution.<br><br>1. How can we see this numerically? Find the [skew across columns](https://www.geeksforgeeks.org/python-pandas-dataframe-skew/) and store the skew in a variable called ${\\color{blue}{\\texttt{the}\\_{\\texttt{skew}}}}$.<br><br>2. What other variables show (from the code below) a fair amount of skew? Can you draw further conclusions from the skew of these anonymised variables?<br><br>$\\color{red}{\\textbf{Answer}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"barplotfraudvsnormal\"></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Plot Fraudulent vs Non-Fraudulent Transactions}}$</h1>\n",
    "\n",
    "### <br><p align=\"justify\" style=\"line-height:25px\">Produce a visual comparison of the number of fraudulent transactions against the non-fraudulent ones. <br><br>As we noted earlier, one of the challenges of this dataset and this project is the highly unbalanced nature of the data we are using. There are (we would hope) far fewer fraudulent transactions than there are non-fraudulent ones and we can really draw attention to this with a bar chart. Use the variable [$\\color{blue}{\\texttt{counts}}$](#Fraudulent-non-fraudulent-transactions) that you defined in [fraudulent and non-fraudulent transactions](#Fraudulent-non-fraudulent-transactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"corr\"></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Pearson Correlation amongst Columns}}$</h1>\n",
    "\n",
    "### <br><p align=\"justify\" style=\"line-height:25px\">Compute the pairwise correlation of columns, excluding $\\color{blue}{\\texttt{NA}}$ and $\\color{blue}{\\texttt{null}}$ values and store it in a variable called $\\color{blue}{\\texttt{corr}}$.<br><br>The [pandas correlation method](<https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html>), $\\color{blue}{\\texttt{corr}}$, defaults to Pearson (i.e., it will calculate the standard pairwise Pearson correlation coefficient), with a minimum number of observations required per pair of columns being $1$ in order to have a valid result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Pearson Correlation Visualization}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Produce a seaborn heatmap of the correlation between every variable in the dataset.<br><br>Set a $\\color{blue}{\\texttt{mask}}$ attribute such that data will not be plotted in cells where $\\color{blue}{\\texttt{mask}}$ is True:<br><br>1. Create a NumPy array, filled with zeros (which in Python also means False), that has the same shape as the $\\color{blue}{\\texttt{corr}}$ array. Since $\\color{blue}{\\texttt{corr}}$ and $\\color{blue}{\\texttt{mask}}$ have the same shape they will line up item-for-item / like-for-like.<br><br>2. Next, set $\\color{blue}{\\texttt{mask}}$ to True for those indicies that lie in the upper triange ($\\color{blue}{\\texttt{triu}\\_{\\texttt{indices}}\\_{\\texttt{from}}}$) of the matrix of correlation values.<br><br>3. Use seaborn's [heatmap method](<http://seaborn.pydata.org/generated/seaborn.heatmap.html#seaborn.heatmap>) to create a matrix plot of the correlation coefficient for each variable in the dataset compared to every other variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Reading the correlation's table and heatmap}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Are there any pairs of variables you feel we could look at in more detail? Is there a non-negligible correlation between some variables? Which ones?<br><br>$\\color{red}{\\textbf{Answer}}$<br><br>Create a [seaborn pairplot](<http://seaborn.pydata.org/generated/seaborn.pairplot.html#seaborn.pairplot>) of those variables (against one another, they should indicate the source of these correlations); use $\\color{blue}{\\texttt{hue}}$ to color the figures based on the categorical variable $\\color{blue}{\\texttt{Class}}$ and $\\color{blue}{\\texttt{markers}}$ to show fraudulent and non-fraudulent transactions with different symbols.<br><br> $\\color{red}{\\textbf{The code cell for this task may take some minutes, please be patient}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Reading the pairplot}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Sure enough we see that for both variables $\\color{red}{\\underline{\\text{answer}}}$ and $\\color{red}{\\underline{\\text{answer}}}$ the correlation against the Transaction $\\color{red}{\\underline{\\text{answer}}}$ variable occurs at values of $\\color{red}{\\underline{\\text{answer}}}$ and $\\color{red}{\\underline{\\text{answer}}}$ equal to $\\color{red}{\\underline{\\text{answer}}}$.<br><br> Again, since the data set has been anonymised it is difficult to draw any further conclusions based on this. It may well be, for example, that the outliers in both variables are fraudulent transactions, given that the correlation has been heavily influenced by values typically around zero. We simply have no further evidence to take such a theory any further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/machinelearning.png\" alt=\"MachineLearning\" style=\"display: auto;margin-left:auto;margin-right: auto;width:150%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Preprocessing}}$</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Scale Transaction Amount and Time}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">When we seek to link predictor variable(s) to response variable(s) it is crucial to consider the scale of the data sets you intend to use in your chosen machine learning algorithm(s).<br><br> Put simply, with a range of predictor variables (independent variables) it will often be the case that the units, and therefore the scales, over which those predictors are measured (i.e their domain) will be different from one predictor variable to the next. This in turn will effect machine learning performance, stability, and error.<br><br>The problem will be compounded if the output variable(s) (i.e. the response variables) are then used in other processes further downstream. Therefore scaling of the machine learning outputs may also be necessary.\n",
    "    \n",
    "<img src=\"Images/scaling.jpeg\" alt=\"Scale\" style=\"display: auto;margin-left:auto;margin-right: auto;width:40%;\"/>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Differences in the scales across input predictor variables may actually increase the difficulty of the problem being modelled ([machinelearningmastery](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)).<br><br>For example, large input values, ranging over hundreds or thousands of units, can result in a model that learns large weight values.<br><br>In turn, a model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.<br><br>On the other hand, an output response variable with a large spread of values may result in large error gradient values causing weight values to change dramatically, again making the learning process unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Scaling input values}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Examples of scaling predictor variables include using small values (such as within the range of $0$ to $1$), and variables that are standardized in some way (such as being transformed to have a zero mean and a standard deviation of one).<br><br>Broadly, we seek to scale input features such that they look more or less like standard, normally distributed data i.e. Gaussian with zero mean and variance ([preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)).<br><br>In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "### $$\\textbf{Standardization:}\\quad z = \\frac{x-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Scale transaction time}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The $\\color{blue}{\\texttt{sklearn.preprocessing}}$ module includes scaling, centering, normalization, binarization and imputation methods.<br><br>API: <https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing><br>Guide: <https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">1. Import the [$\\color{blue}{\\texttt{StandardScaler}}$](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) object from scikit-learn's [$\\color{blue}{\\texttt{preprocessing}}$](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) module and initialize it into a variable named $\\color{blue}{\\texttt{scaler}}$. The [$\\color{blue}{\\texttt{StandardScaler}}$](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) object standardizes features by removing the mean and scaling to unit variance.<br/><br/>2. Scale the transaction time input with the [$\\color{blue}{\\texttt{fit}\\_{\\texttt{transform}}}$](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform) method  into a variable named $\\color{blue}{\\texttt{scaled}\\_{\\texttt{time}}}$, this will create a 2D NumPy array.<br><br>Note that we use double brackets<a id=\"doublebrackets\"></a> to extract the variable of interest from the original DataFrame - using double square brackets ensures you return a pandas DataFrame as the result (rather than a pandas Series).<br><br>3. Extract individual scaled transaction time entries (of which there are 284807) from the $\\color{blue}{\\texttt{scaled}\\_{\\texttt{time}}}$ 2D array and rebuild them into a new generic Python list variable (again, with length 284807, only this time it will only be 1D):<br><br>$\\quad$a) Convert de NumPy array $\\color{blue}{\\texttt{scaled}\\_{\\texttt{time}}}$ into a generic Python list with the NumPy [$\\color{blue}{\\texttt{to}\\_{\\texttt{list}}}$](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ndarray.tolist.html) attribute.<br><br>$\\quad$b) Nest the above in a [$\\color{blue}{\\texttt{for}\\dots{\\texttt{in}}}$](https://docs.python.org/3/tutorial/controlflow.html#for-statements) construct.<br><br>In this way we can extract each item (that is, each scaled transaction time entry) in turn, creating a $\\color{blue}{\\texttt{sublist}}$, which should be a generic 1D Python list of individual scaled transaction time values.<br><br>$\\quad$c) Iterate through the items of $\\color{blue}{\\texttt{sublist}}$, taking each one individually and building a new generic 1D Python list, <br>$\\quad\\,\\,\\,\\,$ called $\\color{blue}{\\texttt{flat}\\_{\\texttt{list}}}$.<br><br>$\\quad$d) Convert the generic Python list of scaled transaction times into a pandas Series object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\begin{align*}\n",
    "\\textbf{scaled_time } & \\textbf{ $($a 2D NumPy array$)$} \\\\\n",
    "&\\downarrow \\\\\n",
    "\\textbf{sublist in scaled_time.tolist() } & \\textsf{ $($a 1D Python list$)$} \\\\\n",
    "&\\downarrow \\\\\n",
    "\\textbf{item in sublist } & \\textbf{ $($Python float$)$} \\\\\n",
    "&\\downarrow \\\\\n",
    "\\textbf{flat_list } & \\textbf{ $($a 1D Python list$)$} \\\\\n",
    "&\\downarrow \\\\\n",
    "\\textbf{scaled_time } & \\textbf{ $($pandas Series$)$}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Scale transaction amount}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Scale the transaction amount column - pretty much identical to just above, where we scaled the transaction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"concatenate\"></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Concatenate scaled inputs}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Create a new DataFrame named $\\color{blue}{\\texttt{df}\\_{\\texttt{with}}\\_{\\texttt{scaling}}}$ by concatenating the scaled transaction time and amount inputs (as created above) with the original DataFrame.<br><br>Use pandas [$\\color{blue}{\\texttt{concat}}$](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) to add the named pandas Series ($\\color{blue}{\\texttt{scaled}\\_{\\texttt{amount}}}$ and $\\color{blue}{\\texttt{scaled}\\_{\\texttt{time}}}$) to the original DataFrame that we read in originally ([$\\color{blue}{\\texttt{df}\\_{\\texttt{orig}}}$](#Original-DataFrame)). Specifically requesting $\\color{blue}{\\texttt{axis=1}}$ means we are adding two new columns.<br><br> Get a grasp of what your data looks like now with a sample of size $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Drop non-scaled inputs from original DataFrame}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Remove the original, non-scaled transaction time and amount columns from the new DataFrame you have just created ($\\color{blue}{\\texttt{df}\\_{\\texttt{with}}\\_{\\texttt{scaling}}}$). In doing so, save the result to yet another new DataFrame named $\\color{blue}{\\texttt{df}\\_{\\texttt{scaled}}}$.<br><br>Use the [$\\color{blue}{\\texttt{drop}}$](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) attribute of the pandas DataFrame $\\color{blue}{\\texttt{df}\\_{\\texttt{with}}\\_{\\texttt{scaling}}}$ to remove the two original columns of Transaction Amount and Time.<br><br> Overall, we will be left with a brand new pandas DataFrame, named $\\color{blue}{\\texttt{df}\\_{\\texttt{scaled}}}$ that contains all the original variables - except for Transaction Time and Amount, which have been replaced by their scaled originals.<br><br> Get a grasp of what your data looks like now with a sample of size $n=5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Training and Testing sets}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">A training set is used to build and validate a learning model, while a testing set is reserved for testing the model on unseen data. The testing set must be large enough to yield statistically meaningful results and must be representative of the dataset as a whole.</p>\n",
    "\n",
    "<img src=\"Images/traintest.png\" alt=\"TrainTest\" style=\"display: auto;margin-left:auto;margin-right: auto;width:80%;\"/>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The goal is to create a model that generalizes well to new data. The testing set serves in place of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Split data into \"train\" and \"test\" sets}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Produce a 1D NumPy array, named $\\color{blue}{\\texttt{mask}}$, randomly filled with values that are either Boolean $\\color{blue}{\\texttt{True}}$ or $\\color{blue}{\\texttt{False}}$, whose size is equal to the number of data points (observations) in our dataset:<br><br>1. Create a NumPy array named $\\color{blue}{\\texttt{mask}}$ of the required shape and populate it with [random samples](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.rand.html) from a uniform distribution over $[0,1)$, but overrule the limit of less than $1$ by applying a new limit of less than $0.9$. In doing so we are effectively applying an $\\color{blue}{\\texttt{IF}}$ condition that states:<br><br><center>If an item is less than $0.9$, it is True $\\dots$ and if it is $0.9$ or above, it is False.</center><br><p align=\"justify\" style=\"line-height:25px\">2. Use this $\\color{blue}{\\texttt{mask}}$ of randomly chosen $\\color{blue}{\\texttt{True}}$ and $\\color{blue}{\\texttt{False}}$ values and apply this to the DataFrame of scaled data ($\\color{blue}{\\texttt{df}\\_{\\texttt{scaled}}}$) to create two new pandas DataFrames (name them $\\color{blue}{\\texttt{train}}$ and $\\color{blue}{\\texttt{test}}$ for $\\color{blue}{\\texttt{True}}$ and $\\color{blue}{\\texttt{False}}$, respectively) containing all columns from the original dataset.<br><br> By virtue of how $\\color{blue}{\\texttt{mask}}$ is created, $\\color{blue}{\\texttt{train}}$ will contain approximately $90\\%$ of the original dataset observations, and $\\color{blue}{\\texttt{test}}$ the approximately $10\\%$ that remain. And the observations in each will have been randomly chosen.<br><br> 3. Print the shape of the training and testing sets and the percentage of observations in each of these sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reset\"></a>\n",
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Reset train and test DataFrame indexes}}$</h1>\n",
    "\n",
    "### <br><p align=\"justify\" style=\"line-height:25px\"> Before moving forward, [reset the index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) of the $\\color{blue}{\\texttt{train}}$ and $\\color{blue}{\\texttt{test}}$ DataFrames, but with the attribute $\\color{blue}{\\texttt{drop}}$ set to $\\color{blue}{\\texttt{True}}$ so that we don't allow the old index to be added as a new column.<br><br>Likewise, the change is made $\\color{blue}{\\texttt{inplace}}$, so that the DataFrames $\\color{blue}{\\texttt{train}}$ and $\\color{blue}{\\texttt{test}}$ end up with new indexes, that count from $0$ up to the number of rows present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Resampling}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> We are dealing with a highly unbalanced dataset in this project. There is a disproportionate ratio of observations between the $\\color{blue}{\\texttt{Fraudulent}}$ and $\\color{blue}{\\texttt{Non-fraudulent}}$ classes and this is a big issue in machine learning classification; it can result in a serious bias towards the $\\color{blue}{\\texttt{Non-fraudulent}}$ class, reducing the classification performance and increasing the number of false negatives.\n",
    "    \n",
    "<img src=\"Images/imbalanced.jpeg\" alt=\"Imbalance\" style=\"display: auto;margin-left:auto;margin-right: auto;width:50%;\"/>\n",
    "    \n",
    "### <p align=\"justify\" style=\"line-height:25px\">Most machine learning algorithms are designed to maximize accuracy and reduce error; therefore, they work best when the training and testing sets have the same number of samples. The most common techniques to deal with this issue are undersampling the majority class, oversampling the minority class or a mix of both. \n",
    "    \n",
    "<img src=\"Images/resampling.png\" alt=\"resampling\" style=\"display: auto;margin-left:auto;margin-right: auto;width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Undersampling}}$</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Undersampling non-fraudulent transactions}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Create a subsample with balanced class distributions.<br><br>1. How many random samples from normal transactions do we need? Count the number of fraudulent transactions in the train data and save it in a variable named $\\color{blue}{\\texttt{no}\\_{\\texttt{of}}\\_{\\texttt{frauds}}}$. $\\color{red}{\\textbf{Hint:}}$ You have counted fraudulent and non-fraudeulent transactions [before](#Fraudulent-non-fraudulent-transactions).<br><br>2. Randomly select $\\color{blue}{\\texttt{no}\\_{\\texttt{of}}\\_{\\texttt{frauds}}}$ non-fraudulent transactions from the train data.<br><br>$\\quad$a) Extract the non-fraudulent and fraudulent transactions from the training set and save<br>$\\quad\\,\\,\\,\\,\\,\\,$them to new DataFrames named $\\color{blue}{\\texttt{non}\\_{\\texttt{fraud}}}$ and $\\color{blue}{\\texttt{fraud}}$, respectively.<br>$\\quad$b) Sample $\\color{blue}{\\texttt{no}\\_{\\texttt{of}}\\_{\\texttt{frauds}}}$ non-fraudulent transactions from the $\\color{blue}{\\texttt{non}\\_{\\texttt{fraud}}}$ DataFrame<br>$\\quad\\,\\,\\,\\,\\,\\,$and save this sample into a new DataFrame named $\\color{blue}{\\texttt{selected}}$.<br><br>3. Concatenate the $\\color{blue}{\\texttt{selected}}$ and $\\color{blue}{\\texttt{fraud}}$ DataFrames into a subsample dataset with equal<br>$\\,\\,\\,\\,$class distribution.<br><br>$\\quad$a) [Reset the index](#reset) of the $\\color{blue}{\\texttt{selected}}$ and $\\color{blue}{\\texttt{fraud}}$ DataFrames.<br>$\\quad$b) [Concatenate](#concatenate) both DatFrames into a subsample dataset named $\\color{blue}{\\texttt{subsample}}$.<br><br>4. Shuffle the $\\color{blue}{\\texttt{subsample}}$ dataset (pretty much like shuffling cards) by using the <br>$\\,\\,\\,\\,$ [sample method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html) from pandas and \"chain\" an [index reset](#reset).\n",
    "    \n",
    "<img src=\"Images/shuffle.jpg\" alt=\"shuffling\" style=\"display: auto;margin-left:auto;margin-right: auto;width:30%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Plot Fraudulent vs Non-Fraudulent Transactions}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Produce a visual comparison of the number of fraudulent transactions against the non-fraudulent ones.<br><br> There are (we would hope) the same number of fraudulent and non-fraudulent transactions and we can really draw attention to this with a bar chart, like the one you created [above](#barplotfraudvsnormal). First you will have to define a new variable $\\color{blue}{\\texttt{new}\\_{\\texttt{counts}}}$ ([see how you did it before](#Fraudulent-non-fraudulent-transactions))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Correlations between Class and the anonymised variables}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">1. Take a look at [correlations once more](#corr), but now in the $\\color{blue}{\\texttt{subsample}}$ dataset. After that, extract the column $\\color{blue}{\\texttt{Class}}$ in a DataFrame named $\\color{blue}{\\texttt{corr}}$ in order to focus only on the correlations between the $\\color{blue}{\\texttt{Class}}$ and the variables V1, V2, $\\dots$<br><br>2. From this DataFrame, extract all variables Vi with negative correlations smaller than $-0.5$.<br><br>3. Now extract all variables Vi with positive correlations greater than $0.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Visualizing features with high negative correlation}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Produce a $m\\times n$ grid of boxplots for the variables with high negative correlation, spliting the $x-$axis into fraudulent and non-fraudulent transactions. Do the same for the variables with high positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Reading the boxplots}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Compare the medians of fraudulent and non-fraudulent transactions from the above boxplots. What can you observe? Does this show that fraudulent transactions contribute more to the negative/positive correlation?<br><br>$\\color{red}{\\textbf{Answer}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Remove extreme outliers}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Sure enough we can see several anomalies (outliers) from the boxplots above. Remove the [extreme outliers](https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm) from the $\\color{blue}{\\texttt{subsample}}$ dataset.<br><br>1. Define the first and third quartile as $\\color{blue}{\\texttt{Q1}}$ and $\\color{blue}{\\texttt{Q3}}$, respectively, using the [pandas function quantile](https://www.geeksforgeeks.org/python-pandas-dataframe-quantile/).<br><br>2. Define the interquartile range as $\\color{blue}{\\texttt{IQR}}$.<br><br>3. Remove extreme outliers ($x<(Q1-3 IQR)$ or $x<(Q1-3 IQR)$) from the $\\color{blue}{\\texttt{subsample}}$ dataset using the [numpy function any](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.any.html) and save this new DataFrame under the name $\\color{blue}{\\texttt{df}\\_{\\texttt{out}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{t-Distributed Stochastic Neighbor Embedding}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">[t-Distributed Stochastic Neighbor Embedding (t-SNE)](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) is a technique for dimensionality reduction that is well suited for the visualization of high-dimensional datasets. The aim of dimensionality reduction is to preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map.\n",
    "    \n",
    "<img src=\"Images/tsne.png\" alt=\"tsne\" style=\"display: auto;margin-left:auto;margin-right: auto;width:30%;\"/>\n",
    "    \n",
    "### <p align=\"justify\" style=\"line-height:25px\">t-SNE is capable of capturing much of the local structure of the high-dimensional data very well, while also revealing global structure such as the presence of clusters at several scales. It uses a Student-t distribution to compute the similarity between two points in the high-dimensional space and in the low-dimensional space. It then tries to optimize these two similarity measures using a cost function.<br><br> You can find many interesting [examples of the use of t-SNE here](https://lvdmaaten.github.io/tsne/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Applying t-SNE}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Let us apply t-SNE to visualise the high-dimensional dataset $\\color{blue}{\\texttt{df}\\_{\\texttt{out}}}$.<br><br>1. Import the [TSNE](https://scikit-learn.org/stable/modules/manifold.html#t-sne) object from scikit-learn's [manifold](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) module and split the $\\color{blue}{\\texttt{df}\\_{\\texttt{out}}}$ DataFrame in two:<br><br>$\\quad$a) Save the $\\color{blue}{\\texttt{Class}}$ column in a pandas Series named $\\color{blue}{\\texttt{y}}$, the dependent variable (now we use single brackets,<br>$\\quad\\,\\,\\,\\,$ [not double like before](#doublebrackets), we are going to use this in the next code cell).<br>$\\quad$b) Save the rest of the variables in a DataFrame named $\\color{blue}{\\texttt{X}}$, the independent variables (just [drop](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) the $\\color{blue}{\\texttt{Class}}$<br>$\\quad\\,\\,\\,\\,$ column).<br><br>2. Run [TSNE](https://scikit-learn.org/stable/modules/manifold.html#t-sne) with two components and setting a [random_state](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) of $42$.<a id=\"randstate\"></a> In machine learning, we want to replicate our steps exactly same as performed before, to analyse the results. Hence $\\color{blue}{\\texttt{random}\\_{\\texttt{state}}}$ is fixed to some integer. Then \"chain\" the [sklearn fit_transform method](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE.fit_transform) to fit the [values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) of $\\color{blue}{\\texttt{X}}$ into a two-dimensional space and return that transformed output. Save it in a variable named $\\color{blue}{\\texttt{X}\\_{\\texttt{reduced}}\\_{\\texttt{tsne}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{t-SNE Scatter Plot}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Create the two-dimensional t-SNE scatter plot and color each transaction by its respective $\\color{blue}{\\texttt{Class}}$ label. We expect fraudulent and non-fraudulent transactions to be very clearly clustered in their own sub groups.<br><br>1. First we will create some color patches, so import patches from matplotlib as mpatches and Create a 'Figure' object and a single 'axes.Axes' object.<br><br>2. Create a blue patch for non-fraudulent transactions and a red patch for the fraudulent transactions using the [Patch method](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.patches.Patch.html#matplotlib.patches.Patch). Save the variables as $\\color{blue}{\\texttt{blue}\\_{\\texttt{patch}}}$ and $\\color{blue}{\\texttt{red}\\_{\\texttt{patch}}}$, respectively.<br><br>3. Create a [scatter plot](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.scatter.html) of the two components of $\\color{blue}{\\texttt{X}\\_{\\texttt{reduced}}\\_{\\texttt{tsne}}}$ and color them using the pandas Series $\\color{blue}{\\texttt{y}}$ that you defined in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Comparison of various Classification Algorithms}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">A classification task is the problem of defining the association between a categorical variable and independent variables that can take either continuous or discrete values. The categorical variable take values among a set of possible classes. In this project we deal with a binary classification problem; there are only two classes: Fradulent and non-fraudulent.<br><br>In a classification problem an algorithm is assessed on its overall accuracy to predict the correct classes of new unseen observations. Furthermore, in an unbalanced classification problem, the [Receiver Operating Characteristics-Area Under the Curve or ROC-AUC](https://www.sciencedirect.com/science/article/abs/pii/S016786550500303X) performance measure is used to evaluate the performance of a classifier. Essentially, the ROC-AUC <a id=\"rocauc\"></a> measure is a value between zero and one, whereby one is a perfect score and zero the worst. An ROC-AUC score of above 0.5 means a higher performance than just random guessing.\n",
    "    \n",
    "<img src=\"Images/ROC.png\" alt=\"ROC\" style=\"display: auto;margin-left:auto;margin-right: auto;width:30%;\"/>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> In the graph above, TPR and FPR stand for True Positive Rate and False Positive Rate, which are defined as:\n",
    "    \n",
    "### $$\\textbf{TPR}=\\frac{TP}{TP+FN}\\qquad\\textbf{FPR}=\\frac{FP}{FP+TN}$$\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">where $TP, FN, FP$ and $TN$ are the numbers of True Positives, False Negatives, False Positives and True Negatives, respectively, given in the confusion matrix:\n",
    "\n",
    "<img src=\"Images/ConfM.png\" alt=\"Confusion Matrix\" style=\"display: auto;margin-left:auto;margin-right: auto;width:20%;\"/>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">In our case, $Y_1$ is the set of fraudulent transactions, $Y_0$ the set of non-fraudulent transactions, $\\hat{Y}_1$ the set of transactions predicted as fraudulent and $\\hat{Y}_0$ the set of transactions predicted as non-fraudulent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Spliting into training and test sets}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">When performing a (supervised) machine learning experiment, we want to hold out part of the available data as a test set $\\color{blue}{\\texttt{X}\\_{\\texttt{test}}}$, $\\color{blue}{\\texttt{y}\\_{\\texttt{test}}}$ and the other part as a training set $\\color{blue}{\\texttt{X}\\_{\\texttt{train}}}$, $\\color{blue}{\\texttt{y}\\_{\\texttt{train}}}$. A random split into training and test sets can be computed with the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from scikit-learn and we can hold out the desired percentage of the data for testing (evaluating) our classifier with $\\color{blue}{\\texttt{Test}\\_{\\texttt{size}}}$.<br><br>1. Import [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from the scikit-learn module [model selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and use it to define the test and training sets<a id=\"trainingset\"></a> $\\color{blue}{\\texttt{X}\\_{\\texttt{test}}}$, $\\color{blue}{\\texttt{y}\\_{\\texttt{test}}}$, $\\color{blue}{\\texttt{X}\\_{\\texttt{train}}}$ and $\\color{blue}{\\texttt{y}\\_{\\texttt{train}}}$.  Hold out $20\\%$ of the data for testing the classifier and a $\\color{blue}{\\texttt{random}\\_{\\texttt{state}}}$ of $42$ as [before](#randstate).<br><br>2. Save the $\\color{blue}{\\texttt{X}\\_{\\texttt{test}}}$ values and the $\\color{blue}{\\texttt{y}\\_{\\texttt{test}}}$ [values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.values.html) in variables named $\\color{blue}{\\texttt{X}\\_{\\texttt{validation}}}$ and $\\color{blue}{\\texttt{X}\\_{\\texttt{validation}}}$, respectively and print the $\\color{blue}{\\texttt{X}}$ and $\\color{blue}{\\texttt{y}}$ shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Classification Algorithms}}$</h1>\n",
    "\n",
    "<h2>$\\color{blue}{\\textbf{Overview}}$</h2>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">The goal of classification is to be able to place a given object into the appropriate category based on data associated with this object. In the present case, the object is the credit card transaction and there are two assigned categories, fraudulent and non-fraudulent.<br><br>Here you can read about the following [classification algorithms in Machine Learning](https://medium.com/@Mandysidana/machine-learning-types-of-classification-9497bd4f2e14): Logistic Regression, Naive Bayes Classifier, Nearest Neighbor, Support Vector Machines, Decision Trees, Boosted Trees, Random Forest and Neural Networks.\n",
    "    \n",
    "<img src=\"Images/Classify.png\" alt=\"Classify\" style=\"display: auto;margin-left:auto;margin-right: auto;width:40%;\"/>\n",
    "\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">There are a lot of classification algorithms available and it is hard, or even impossible, to conclude which one is superior to other. It depends on the application and nature of available data set.<br><br>Now let us compare some classifiers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Importing Classification Algorithms and metrics}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\">Run the following cell to import all the models and metrics you need to complete the case study into the Jupyter Notebook. All these imports are from the [scikit-learn library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install xgboost\n",
    "except:\n",
    "    raise Exception(\"SOME OTHER ERROR WITH LOADING XGBOOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Comparing machine learning algorithms}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> It is crucial to compare the performance of different machine learning algorithms consistently. Therefore, in the following cell, you will create a test harness to compare multiple different machine learning algorithms in Python with scikit-learn. Summarize the results both numerically and using boxplots<br><br>1. Create a list for appending the models, name it $\\color{blue}{\\texttt{models}}$.<br><br>2. Create, configure and append the models: [Append](https://docs.python.org/3/tutorial/datastructures.html) the following algorithms to the $\\color{blue}{\\texttt{models}}$' list: $\\color{blue}{\\texttt{LogisticRegression}}$, $\\color{blue}{\\texttt{LinearDiscriminantAnalysis}}$, $\\color{blue}{\\texttt{KNeighborsClassifier}}$, $\\color{blue}{\\texttt{DecisionTreeClassifier}}$, $\\color{blue}{\\texttt{SVC}}$, $\\color{blue}{\\texttt{XGBClassifier}}$ and $\\color{blue}{\\texttt{RandomForestClassifier}}$.<br><br>3. Test the models.<br><br>$\\quad$a) Create two lists named $\\color{blue}{\\texttt{results}}$ and $\\color{blue}{\\texttt{names}}$.<br><br>$\\quad$b) Then, for $\\color{blue}{\\texttt{name}}$, $\\color{blue}{\\texttt{model}}$ in $\\color{blue}{\\texttt{models}}$, use a [K-Folds cross-validator](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) with $10$ folds to evaluate each algorithm,<br>$\\quad\\,\\,\\,\\,\\,$name it $\\color{blue}{\\texttt{kfold}}$. Configure it with the same [random state as before](#randstate) to ensure that the same splits to the<br>$\\quad\\,\\,\\,\\,$ training data are performed and that each algorithm is evaluated in precisely the same way.<br><br>$\\quad$c) Use the [cross_val_score from](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function from $\\color{blue}{\\texttt{sklearn.model}\\_{\\texttt{selection}}}$ to evaluate a score (by cross<br>$\\quad\\,\\,\\,\\,\\,$validation) of each $\\color{blue}{\\texttt{model}}$ in the [training set](#trainingset) $\\color{blue}{\\texttt{X}\\_{\\texttt{train}}}$, $\\color{blue}{\\texttt{y}\\_{\\texttt{train}}}$. Use $\\color{blue}{\\texttt{cv}}$ as the $\\color{blue}{\\texttt{kfold}}$ defined above and the <br>$\\quad\\,\\,\\,\\,\\,\\color{blue}{\\texttt{roc}\\_{\\texttt{auc}}}$ measure ([ROC-AUC](#rocauc)) for $\\color{blue}{\\texttt{scoring}}$. Save this part in a variable named $\\color{blue}{\\texttt{cv}\\_{\\texttt{results}}}$ and [append](https://docs.python.org/3/tutorial/datastructures.html) it to<br>$\\quad\\,\\,\\,\\,\\,$the $\\color{blue}{\\texttt{results}}$' list (to be used in the next cell). [Append](https://docs.python.org/3/tutorial/datastructures.html) each name in the $\\color{blue}{\\texttt{names}}$' list as well. Finally, print the<br>$\\quad\\,\\,\\,\\,\\,$numerical summary of the results: (name, mean accuracy, standard deviation accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/broke.jpg\" width=\"100\" align=\"Left\"/> \n",
    "<h1>$\\,\\,\\,\\color{blue}{\\textbf{Visual Comparison of th machine learning algorithms}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Produce boxplots showing the spread of the accuracy scores across each cross validation fold for each of the above algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Conclusion}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> Among the algorithms that you tested, what is most efficient algorithm for fraud detection? Why? Use the numerical and graphical summaries to compare the algorithms.<br><br>$\\color{red}{\\textbf{Answer}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>$\\color{blue}{\\textbf{Further tasks for home (not to hand in)}}$</h1>\n",
    "\n",
    "### <p align=\"justify\" style=\"line-height:25px\"> We used the undersampling technique, but there are many other techniques such as oversampling, SMOTE, CNN, ENN, NCL, OSS and Tomek Link. The task for you now, is to assess the efficiency of the other techniques and perhaps, a mix of undersampling and oversampling techniques.<br><h2>Have fun!</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
